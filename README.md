<div align="center">

# ğŸ¯ NumEdge

### *Machine Learning That Makes Sense*

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/downloads/)
[![MIT License](https://img.shields.io/badge/license-MIT-green.svg?style=for-the-badge)](LICENSE)
[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=for-the-badge&logo=numpy&logoColor=white)](https://numpy.org/)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=for-the-badge)](http://makeapullrequest.com)

**A lightweight, intelligent machine learning library built on pure NumPy**

*Where transparency meets power, and learning meets doing*

---

### [ğŸ“¦ Install](#-installation) â€¢ [ğŸš€ Quick Start](#-quick-start) â€¢ [âœ¨ Features](#-core-philosophy) â€¢ [ğŸ¤ Contribute](#-contributing)

---

</div>

<br>

## ğŸŒŸ Why NumEdge Exists

Machine learning shouldn't feel like magic. It should be **transparent**, **intuitive**, and **intelligent**.

NumEdge was born from a simple belief: **great ML tools should teach you while you build**. Every algorithm is implemented in pure NumPy and Pythonâ€”no hidden layers, no cryptic C extensions, just clean, readable code that helps you understand what's really happening under the hood.

<div align="center">

### ğŸ“ Built For

| Students & Learners | Data Scientists | Researchers | Educators |
|:---:|:---:|:---:|:---:|
| Pure Python code you can actually read | Tabular-first with DataFrame support | Reproducible & well-documented | Perfect teaching tool |

</div>

<br>

---

<br>

## âœ¨ Core Philosophy

<table>
<tr>
<td width="33%" align="center">

### ğŸ” **Transparent by Design**

Every algorithm written in pure NumPy/Python. Open any file and understand exactly how the math works. No black boxes, no magic.

</td>
<td width="33%" align="center">

### ğŸ›¡ï¸ **Intelligent Warnings**

Built-in safeguards catch common mistakes before they become bugs. Data leakage? Wrong evaluation? Missing random state? We've got you covered.

</td>
<td width="33%" align="center">

### ğŸ“Š **Tabular-First**

Real data comes in CSVs and DataFrames. NumEdge handles mixed types, preprocessing, and encoding automaticallyâ€”no pipelines required.

</td>
</tr>
</table>

<br>

<div align="center">

### ğŸ¯ *Readable over Fast â€¢ Understanding over Optimization â€¢ Clarity over Complexity*

</div>

<br>

---

<br>

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/Nitin-Prata/numedge.git
cd numedge

# Install in development mode
pip install -e .
```

> ğŸ“¦ **Coming Soon:** `pip install numedge`

**Requirements:**
- Python 3.8 or higher
- NumPy (core dependency)
- pandas (optional, for tabular features)

<br>

---

<br>

## ğŸ’¡ Examples

### ğŸ”¹ Linear Regression

```python
from numedge.models.linear_models import LinearRegression
from numedge.model_selection import train_test_split
import numpy as np

# Generate sample data
X = np.random.randn(1000, 5)
y = X @ np.array([1.5, -2.0, 0.5, 3.0, -1.0]) + np.random.randn(1000) * 0.1

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
model = LinearRegression()
model.fit(X_train, y_train)

# Evaluate
print(f"Training RÂ²: {model.score(X_train, y_train):.4f}")
print(f"Testing RÂ²: {model.score(X_test, y_test):.4f}")

# Make predictions
predictions = model.predict(X_test)
```

<br>

### ğŸ”¹ Random Forest with Hyperparameter Search

```python
from numedge.models.ensemble import RandomForestClassifier
from numedge.model_selection import GridSearchCV

# Create model
rf = RandomForestClassifier(random_state=42)

# Get recommended hyperparameter search space
search_space = rf.get_search_space()
print(f"Recommended search space: {search_space}")

# Perform grid search
grid_search = GridSearchCV(
    estimator=rf,
    param_grid=search_space,
    cv=5,
    scoring='accuracy'
)

grid_search.fit(X_train, y_train)

# Best model
best_model = grid_search.best_estimator_
print(f"Best parameters: {grid_search.best_params_}")
print(f"Best CV score: {grid_search.best_score_:.4f}")
```

<br>

### ğŸ”¹ Tabular Data (DataFrames)

```python
import pandas as pd
from numedge.tabular import TabularClassifier
from numedge.models.ensemble import GradientBoostingClassifier

# Your real-world DataFrame with mixed types
df = pd.DataFrame({
    'age': [25, 35, 45, 22, 55],
    'income': [50000, 75000, 90000, 45000, 120000],
    'city': ['NYC', 'LA', 'NYC', 'Chicago', 'LA'],
    'education': ['Bachelor', 'Master', 'PhD', 'Bachelor', 'Master'],
    'purchased': [0, 1, 1, 0, 1]
})

# Create tabular classifier
model = TabularClassifier(
    estimator=GradientBoostingClassifier(random_state=42),
    target='purchased'
)

# NumEdge automatically:
# âœ… Detects numeric vs categorical columns
# âœ… Scales numeric features
# âœ… One-hot encodes categorical features
# âœ… Handles train/test consistency

model.fit(df)
predictions = model.predict(df)
```

<br>

### ğŸ”¹ K-Means Clustering

```python
from numedge.cluster import KMeans
import matplotlib.pyplot as plt

# Create clusters
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X)

# Get cluster assignments
labels = kmeans.predict(X)
centers = kmeans.cluster_centers_

# Visualize
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.6)
plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=200, edgecolors='black')
plt.title('K-Means Clustering')
plt.show()
```

<br>

### ğŸ”¹ Incremental Learning (Streaming)

```python
from numedge.models.linear_models import SGDRegressor

# Initialize model
model = SGDRegressor(learning_rate=0.01)

# Learn from data in batches (useful for large datasets)
for batch_X, batch_y in data_stream:
    model.partial_fit(batch_X, batch_y)

# Final predictions
final_predictions = model.predict(X_test)
```

<br>

---

<br>

## ğŸ§  Available Algorithms

<details>
<summary><b>ğŸ“ˆ Supervised Learning</b></summary>

<br>

**Linear Models**
- `LinearRegression` â€” Ordinary least squares
- `Ridge` â€” L2 regularized regression
- `Lasso` â€” L1 regularized regression
- `ElasticNet` â€” Combined L1 + L2 regularization
- `LogisticRegression` â€” Binary & multiclass classification
- `SGDRegressor` â€” Stochastic gradient descent regression
- `SGDClassifier` â€” Stochastic gradient descent classification

**Tree-Based Models**
- `DecisionTreeClassifier` â€” CART algorithm
- `DecisionTreeRegressor` â€” Regression trees
- `RandomForestClassifier` â€” Ensemble of decision trees
- `RandomForestRegressor` â€” Ensemble for regression
- `ExtraTreesClassifier` â€” Extremely randomized trees
- `ExtraTreesRegressor` â€” Extra trees for regression

**Ensemble Methods**
- `GradientBoostingClassifier` â€” Gradient boosting
- `GradientBoostingRegressor` â€” Boosting for regression
- `AdaBoostClassifier` â€” Adaptive boosting
- `AdaBoostRegressor` â€” AdaBoost for regression
- `BaggingClassifier` â€” Bootstrap aggregating
- `BaggingRegressor` â€” Bagging for regression

**Support Vector Machines**
- `SVC` â€” Support vector classification
- `SVR` â€” Support vector regression

**Neighbors**
- `KNeighborsClassifier` â€” K-nearest neighbors classification
- `KNeighborsRegressor` â€” K-nearest neighbors regression

**Naive Bayes**
- `GaussianNB` â€” Gaussian Naive Bayes
- `MultinomialNB` â€” Multinomial Naive Bayes

**Advanced Boosting**
- `XGBClassifier` â€” NumPy-based XGBoost implementation
- `XGBRegressor` â€” XGBoost for regression

</details>

<details>
<summary><b>ğŸ” Unsupervised Learning</b></summary>

<br>

**Clustering**
- `KMeans` â€” K-means clustering
- `DBSCAN` â€” Density-based clustering
- `AgglomerativeClustering` â€” Hierarchical clustering

**Dimensionality Reduction**
- `PCA` â€” Principal component analysis

</details>

<details>
<summary><b>âš™ï¸ Preprocessing & Utilities</b></summary>

<br>

**Scalers**
- `StandardScaler` â€” Standardize features (zero mean, unit variance)
- `MinMaxScaler` â€” Scale features to a range
- `RobustScaler` â€” Scale using median and IQR

**Encoders**
- `OneHotEncoder` â€” One-hot encode categorical features
- `LabelEncoder` â€” Encode labels as integers

**Model Selection**
- `train_test_split` â€” Split data into train/test sets
- `cross_val_score` â€” K-fold cross-validation
- `GridSearchCV` â€” Exhaustive hyperparameter search
- `RandomizedSearchCV` â€” Randomized hyperparameter search

**Metrics**
- Classification: `accuracy`, `precision`, `recall`, `f1_score`, `roc_auc`
- Regression: `r2_score`, `mse`, `mae`, `rmse`

</details>

<br>

---

<br>

## ğŸ“ Project Structure

```
numedge/
â”‚
â”œâ”€â”€ ğŸ“‚ src/numedge/
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ¯ core/                    # Base classes, mixins, optimizers
â”‚   â”‚   â”œâ”€â”€ base.py                 # BaseEstimator
â”‚   â”‚   â”œâ”€â”€ mixins.py               # ClassifierMixin, RegressorMixin
â”‚   â”‚   â””â”€â”€ optimizers.py           # Gradient descent variants
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ¤– models/                  # All supervised algorithms
â”‚   â”‚   â”œâ”€â”€ linear_models/          # Linear regression, Ridge, Lasso, etc.
â”‚   â”‚   â”œâ”€â”€ ensemble/               # Random Forest, Boosting, Bagging
â”‚   â”‚   â”œâ”€â”€ tree/                   # Decision Trees
â”‚   â”‚   â”œâ”€â”€ svm/                    # Support Vector Machines
â”‚   â”‚   â”œâ”€â”€ neighbors/              # K-Nearest Neighbors
â”‚   â”‚   â””â”€â”€ naive_bayes/            # Naive Bayes variants
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ” cluster/                 # Clustering algorithms
â”‚   â”‚   â”œâ”€â”€ kmeans.py
â”‚   â”‚   â”œâ”€â”€ dbscan.py
â”‚   â”‚   â””â”€â”€ hierarchical.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“Š decomposition/           # Dimensionality reduction
â”‚   â”‚   â””â”€â”€ pca.py
â”‚   â”‚
â”‚   â”œâ”€â”€ âš™ï¸ preprocessing/           # Data transformers
â”‚   â”‚   â”œâ”€â”€ scalers.py
â”‚   â”‚   â””â”€â”€ encoders.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ² model_selection/         # CV, search, split utilities
â”‚   â”‚   â”œâ”€â”€ split.py
â”‚   â”‚   â”œâ”€â”€ cross_validation.py
â”‚   â”‚   â””â”€â”€ search.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ metrics/                 # Evaluation metrics
â”‚   â”‚   â”œâ”€â”€ classification.py
â”‚   â”‚   â””â”€â”€ regression.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‹ tabular/                 # DataFrame helpers
â”‚   â”‚   â”œâ”€â”€ classifier.py
â”‚   â”‚   â””â”€â”€ regressor.py
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ› ï¸ utils/                   # Internal utilities
â”‚       â”œâ”€â”€ validation.py
â”‚       â”œâ”€â”€ checks.py
â”‚       â””â”€â”€ warnings.py
â”‚
â”œâ”€â”€ ğŸ§ª tests/                       # Comprehensive test suite
â”œâ”€â”€ ğŸ“š examples/                    # Jupyter notebooks & tutorials
â”œâ”€â”€ ğŸ“– docs/                        # Documentation (coming soon)
â””â”€â”€ ğŸ“„ README.md                    # You are here!
```

<br>

---

<br>

## ğŸ—ºï¸ Roadmap

<table>
<tr>
<td width="50%">

### âœ… Current Focus
- [x] Core algorithms implementation
- [x] Tabular data support
- [x] Hyperparameter search spaces
- [x] Intelligent warning system
- [ ] Complete test coverage (>90%)
- [ ] Performance benchmarks

</td>
<td width="50%">

### ğŸ”® Coming Soon
- [ ] Full documentation site
- [ ] PyPI release
- [ ] Interactive tutorials
- [ ] More ensemble methods
- [ ] Advanced feature engineering
- [ ] CI/CD pipeline

</td>
</tr>
</table>

<br>

---

<br>

## ğŸ¤ Contributing

NumEdge is **actively developed** and we'd love your help!

### ğŸŒŸ Ways to Contribute

- ğŸ› **Report Bugs** â€” Found an issue? [Open an issue](https://github.com/Nitin-Prata/numedge/issues)
- ğŸ’¡ **Suggest Features** â€” Have ideas? [Start a discussion](https://github.com/Nitin-Prata/numedge/discussions)
- ğŸ“– **Improve Docs** â€” Better explanations, examples, tutorials
- âœ¨ **Submit Code** â€” New algorithms, optimizations, fixes
- â­ **Star the Repo** â€” Show your support!

Before contributing code, please read our [**Contributing Guidelines**](CONTRIBUTING.md).

<br>

---

<br>

## ğŸ“„ License

NumEdge is open-source software licensed under the **MIT License**.

See the [LICENSE](LICENSE) file for full details.

<br>

---

<br>

## ğŸ‘¨â€ğŸ’» Creator

<div align="center">

### **Nitin Pratap Singh**

[![GitHub](https://img.shields.io/badge/GitHub-@Nitin--Prata-181717?style=for-the-badge&logo=github)](https://github.com/Nitin-Prata)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Nitin%20Singh-0A66C2?style=for-the-badge&logo=linkedin)](https://www.linkedin.com/in/nitin-singh-bb7907298/)
[![X](https://img.shields.io/badge/X-@prata42085-000000?style=for-the-badge&logo=x)](https://x.com/prata42085)
[![Email](https://img.shields.io/badge/Email-nitinpratap997@gmail.com-D14836?style=for-the-badge&logo=gmail&logoColor=white)](mailto:nitinpratap997@gmail.com)

</div>

<br>

---

<br>

## ğŸ™ Acknowledgments

NumEdge stands on the shoulders of giants. Inspired by the open-source ML community and driven by a passion for transparent, educational tools.

Special thanks to everyone who believes that **understanding how things work** is just as important as making them work.

<br>

---

<br>

<div align="center">

### ğŸ’– Made with passion for the ML community

**If NumEdge helps you learn or build something awesome, please consider starring the repo!**

â­ **Star** â€¢ ğŸ´ **Fork** â€¢ ğŸ“£ **Share**

---

[ğŸ› Report Bug](https://github.com/Nitin-Prata/numedge/issues) â€¢ [âœ¨ Request Feature](https://github.com/Nitin-Prata/numedge/issues) â€¢ [ğŸ’¬ Discuss](https://github.com/Nitin-Prata/numedge/discussions)

---

**NumEdge** â€” *Machine Learning That Makes Sense*

</div>